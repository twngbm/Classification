# Classification
## Step1 設計規則並產生資料  
### 規則設計
在此實作中，設計一銀行發行信用卡之機制，銀行透過決策樹，檢查辦卡人資格，並決定是否發卡。  
提出以下幾點變數  

變數   | 閥值  | 
-------|:-----:|
年收入  |>=90萬 | 
固定工作| >=5年 |
信用狀態| 0,1  |
公司前景| 0,1  |
年齡    |>=30歲|
房地產有無|0,1   |

詳細的**絕對規則**決策樹如下圖所示  
![image](https://github.com/twngbm/Classification/blob/master/Right_DT.png)

1代表發卡，-1則不發卡  
### 資料產生
Data_Generator.py為產生亂數資料之檔案 
1. 以亂數的方法來產生資料集，例:為保持資料平衡性，年齡產生區間為20~40歲  
2. 將亂數產生的資料，放入**絕對規則**決策樹中，並藉由決策樹規則產生lable  
3. 將資料分為訓練用與測試用，比例為4:1。  
## Step2 訓練模型
決策樹模型，採用sklearn的決策樹資料結構來進行訓練，訓練總數為40000筆資料。  
訓練檔案為data_train.npy與lable_train.npy，透過Decision_Tree.py進行訓練
## Step3 資料比較
將訓練完的模型，透過圖形呈現  
![image](https://github.com/twngbm/Classification/blob/master/irs.jpg)  

可以發現，透過資料訓練出的模型，與透過規則決定的模型有所差異  
1. gini為異質度，為0時，不存在相異的變數
2. samples為該節點目前存在之資料數

## Step4 討論
在Dicision_Tree.py檔案的後面，在決策樹被訓練完成後，將test的資料放入決策樹進行驗證，發現所計算出的錯誤率為0，可能推測為以下原因。
1. 在訓練過程中可能有overfitting的現象，可以觀察決策樹中，所有末端節點的Gini皆為0。
2. 也有可能是決策樹結構過於簡單，變數過少，造成可以輕易地分類。
3. 測試的資料與訓練的資料皆是透同一套亂數機制產生，可能會使數據過於集中。
4. 亂數產生的數據被我主觀的限制在一個範圍內，原本是為了避免數據分布過廣造成lable不平衡的現象，但是這也有可能是造成反效果的原因之一。
